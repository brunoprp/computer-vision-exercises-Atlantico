{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from datasets import PascalVOCDataset\n",
    "from tqdm import tqdm\n",
    "from pprint import PrettyPrinter\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good formatting when printing the APs for each class and mAP\n",
    "pp = PrettyPrinter()\n",
    "\n",
    "# Parameters\n",
    "data_folder = '/media/bruno/HD-Arquivos2/Data_Object_Detect/' # Path data preparation\n",
    "keep_difficult = True  # difficult ground truth objects must always be considered in mAP calculation, because these objects DO exist!\n",
    "batch_size = 50\n",
    "workers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = 'checkpoint_ssd300.pth.tar' # Path model save\n",
    "\n",
    "# Load model checkpoint that is to be evaluated\n",
    "checkpoint = torch.load(checkpoint)\n",
    "model = checkpoint['model']\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Load test data\n",
    "test_dataset = PascalVOCDataset(data_folder,\n",
    "                                split='test',\n",
    "                                keep_difficult=keep_difficult)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                          collate_fn=test_dataset.collate_fn, num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model with Mean Average Precision [mAP](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader, model):\n",
    "    \"\"\"\n",
    "    Evaluate.\n",
    "\n",
    "    :param test_loader: DataLoader for test data\n",
    "    :param model: model\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure it's in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Lists to store detected and true boxes, labels, scores\n",
    "    det_boxes = list()\n",
    "    det_labels = list()\n",
    "    det_scores = list()\n",
    "    true_boxes = list()\n",
    "    true_labels = list()\n",
    "    true_difficulties = list()  # it is necessary to know which objects are 'difficult', see 'calculate_mAP' in utils.py\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (images, boxes, labels, difficulties) in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
    "            images = images.to(device)  # (N, 3, 300, 300)\n",
    "\n",
    "            # Forward prop.\n",
    "            predicted_locs, predicted_scores = model(images)\n",
    "\n",
    "            # Detect objects in SSD output\n",
    "            det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(predicted_locs, predicted_scores,\n",
    "                                                                                       min_score=0.01, max_overlap=0.45,\n",
    "                                                                                       top_k=200)\n",
    "            # Evaluation MUST be at min_score=0.01, max_overlap=0.45, top_k=200 for fair comparision with the paper's results and other repos\n",
    "\n",
    "            # Store this batch's results for mAP calculation\n",
    "            boxes = [b.to(device) for b in boxes]\n",
    "            labels = [l.to(device) for l in labels]\n",
    "            difficulties = [d.to(device) for d in difficulties]\n",
    "\n",
    "            det_boxes.extend(det_boxes_batch)\n",
    "            det_labels.extend(det_labels_batch)\n",
    "            det_scores.extend(det_scores_batch)\n",
    "            true_boxes.extend(boxes)\n",
    "            true_labels.extend(labels)\n",
    "            true_difficulties.extend(difficulties)\n",
    "\n",
    "        # Calculate mAP\n",
    "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n",
    "\n",
    "    # Print AP for each class\n",
    "    pp.pprint(APs)\n",
    "    return (APs, mAP)\n",
    "    print('\\nMean Average Precision (mAP): %.3f' % mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaluate\n",
    "(APs, mAp) = evaluate(test_loader, model)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6427056789398193"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result mAP model\n",
    "mAp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aeroplane': 0.708512008190155,\n",
       " 'bicycle': 0.7454192638397217,\n",
       " 'bird': 0.6230981945991516,\n",
       " 'boat': 0.5221483111381531,\n",
       " 'bottle': 0.24800018966197968,\n",
       " 'bus': 0.7201552391052246,\n",
       " 'car': 0.7631471753120422,\n",
       " 'cat': 0.8401749730110168,\n",
       " 'chair': 0.3495808243751526,\n",
       " 'cow': 0.703272819519043,\n",
       " 'diningtable': 0.6119518876075745,\n",
       " 'dog': 0.7849552035331726,\n",
       " 'horse': 0.8075953722000122,\n",
       " 'motorbike': 0.7407485842704773,\n",
       " 'person': 0.658084511756897,\n",
       " 'pottedplant': 0.26374727487564087,\n",
       " 'sheep': 0.6770304441452026,\n",
       " 'sofa': 0.6690605878829956,\n",
       " 'train': 0.7741146087646484,\n",
       " 'tvmonitor': 0.6433150172233582}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mAP for each class\n",
    "APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
